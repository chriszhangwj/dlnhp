# -*- coding: utf-8 -*-
"""Baseline_code_tune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rIULTFgUR9PfJWz-rN7EwCr6w0Ywpo_W

# Baseline code
This code introduces a two-step training for the problem. It may be better doing only one-step, i.e. from noisy patch to descriptor directly, but this provides an initial valid submission to use as a first step.

The outputs you see here are with only some minutes of training, so results should be better if the models are trained for more time.

## Initial check

We first check the amount of memory we have in the notebook. Usually, we have available 11.4 GB of GPU memory, which is more than enough to run this code. However, some users reported having only 500 MB of GPU memory. If you have that amount, restart the environment to see if you get the corresponding 11.4 GB
"""

# Taken from
# https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available
# memory footprint support libraries/code
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip install gputil
!pip install psutil
!pip install humanize
import psutil
import humanize
import os
import GPUtil as GPU
GPUs = GPU.getGPUs()
# XXX: only one GPU on Colab and isnâ€™t guaranteed
gpu = GPUs[0]
def printm():
  process = psutil.Process(os.getpid())
  print("Gen RAM Free: " + humanize.naturalsize( psutil.virtual_memory().available ), " | Proc size: " + humanize.naturalsize( process.memory_info().rss))
  print("GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))

printm()

"""## Downloading functions and data

The first step is to clone a GitHub repository with some functions implemented, and also downloading and extracting the HPatches data. We can run command line commands by using ```!```. Also, by using ```%``` we have access to the [built-in IPython magic commands](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-cd), which we use to change directory (`cd`). It takes around 5 minutes to download and unzip the dataset.
"""

# Clone repo
!git clone https://github.com/MatchLab-Imperial/keras_triplet_descriptor

# Change directory
# %cd /content/keras_triplet_descriptor

# Download data
# !wget -O hpatches_data.zip https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip

# Extract data
# !unzip -q ./hpatches_data.zip
# !rm ./hpatches_data.zip

"""## Importing necessary modules

We now import the modules we will use in this baseline code. The read_data and utils imports are function provided in the repository we just cloned.
"""

import sys
import json
import os
import glob
import keras
from keras import backend as K
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Activation 
from keras.layers import Input, UpSampling2D, concatenate  
import time
import tensorflow as tf
import numpy as np
import cv2
import random
from read_data import HPatches, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, tps
from utils import generate_desc_csv, plot_denoise, plot_triplet

"""We also fix the seeds of the pseudo-random number generators to have reproducible results. The idea of fixing the seed is having the same results every time the algorithm is run if there are no changes in the code."""

random.seed(1234)
np.random.seed(1234)
tf.set_random_seed(1234)
# %pwd

"""The HPatches dataset has several splits, where it separates the sequences available in train sequences and test sequences. We load the split 'a'."""

hpatches_dir = './hpatches'
splits_path = './splits.json'

splits_json = json.load(open(splits_path, 'rb'))
split = splits_json['a']

train_fnames = split['train']
test_fnames = split['test']

seqs = glob.glob(hpatches_dir+'/*')
seqs = [os.path.abspath(p) for p in seqs]   
seqs_train = list(filter(lambda x: x.split('/')[-1] in train_fnames, seqs)) 
seqs_test = list(filter(lambda x: x.split('/')[-1] in test_fnames, seqs))

"""## Models and loss

We now define three functions that define main modules of our baseline. First, we have a function that returns a denoising model. The input for the function is the size of the patch, which will be 1x64x64, and it outputs a keras model.

Then we have a similar function for the descriptor model, the model we use as baseline takes as input a patch of size 1x32x32, and returns a descriptor. Then we will use the triplet loss.

You can modify the models in this functions and run the training code again for your new models. For example, the given UNet is quite shallow, maybe using a deeper network can improve results. Or testing new initializations for the weigths. Or maybe adding dropout. Or modifying the loss somehow....
"""

def get_denoise_model(shape):

    inputs = Input(shape)
    if mode==0:  # shallow
      conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)
      pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
      ## Bottleneck
      conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)

      ## Now the decoder starts
      up3 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv2))
      merge3 = concatenate([conv1,up3], axis = -1)
      conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge3)
      conv4 = Conv2D(1, 3,  padding = 'same')(conv3)
      
      model = Model(input = inputs, output = conv4)
      
    else if mode==1 # shallow BN
      conv1 = Conv2D(16, 3, padding = 'same', kernel_initializer = 'he_normal')(inputs)
      bm1 = BatchNormalization(axis=-1)(conv1)
      activ1 = Activation('relu')(bm1)
      pool1 = MaxPooling2D(pool_size=(2, 2))(activ1)
      
      ## Bottleneck
      conv2 = Conv2D(32, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)
      bm2 = BatchNormalization(axis=-1)(conv2)
      activ2 = Activation('relu')(bm2)

      ## Now the decoder starts
      up3 = Conv2D(64, 2, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(activ2))
      bm3 = BatchNormalization(axis=-1)(up3)
      activ3 = Activation('relu')(bm3)
      merge3 = concatenate([activ1,activ3], axis = -1)
      conv3 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(merge3)
      bm4 = BatchNormalization(axis=-1)(conv3)
      activ4 = Activation('relu')(bm4)
      conv4 = Conv2D(1, 3,  padding = 'same')(activ4)
      
      model = Model(input = inputs, output = conv4)
      
    else if mode==2: # u-net
      conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)
      conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)
      pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
      conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)
      conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)
      pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
      conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)
      conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)
      pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
      conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)
      conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)
      drop4 = Dropout(0.5)(conv4)
      pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)

      conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)
      conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)
      drop5 = Dropout(0.5)(conv5)

      up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))
      merge6 = concatenate([drop4,up6], axis = 3)
      conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)
      conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)

      up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))
      merge7 = concatenate([conv3,up7], axis = 3)
      conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)
      conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)

      up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))
      merge8 = concatenate([conv2,up8], axis = 3)
      conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)
      conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)

      up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))
      merge9 = concatenate([conv1,up9], axis = 3)
      conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)
      conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
      conv9 = Conv2D(2, 3, padding = 'same')(conv9)

      model = Model(input = inputs, output = conv9)
      
    else if mode==2:
      
    
  return model


def get_descriptor_model(shape):
  '''Architecture copies HardNet architecture'''
  init_weights = keras.initializers.he_normal()
  descriptor_model = Sequential()
  descriptor_model.add(Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights))
  descriptor_model.add(BatchNormalization(axis = -1))
  descriptor_model.add(Activation('relu'))

  descriptor_model.add(Conv2D(32, 3, padding='same', use_bias = True, kernel_initializer=init_weights))
  descriptor_model.add(BatchNormalization(axis = -1))
  descriptor_model.add(Activation('relu'))

  descriptor_model.add(Conv2D(64, 3, padding='same', strides=2, use_bias = True, kernel_initializer=init_weights))
  descriptor_model.add(BatchNormalization(axis = -1))
  descriptor_model.add(Activation('relu'))

  descriptor_model.add(Conv2D(64, 3, padding='same', use_bias = True, kernel_initializer=init_weights))
  descriptor_model.add(BatchNormalization(axis = -1))
  descriptor_model.add(Activation('relu'))

  descriptor_model.add(Conv2D(128, 3, padding='same', strides=2,  use_bias = True, kernel_initializer=init_weights))
  descriptor_model.add(BatchNormalization(axis = -1))
  descriptor_model.add(Activation('relu'))

  descriptor_model.add(Conv2D(128, 3, padding='same', use_bias = True, kernel_initializer=init_weights))
  descriptor_model.add(BatchNormalization(axis = -1))
  descriptor_model.add(Activation('relu'))
  descriptor_model.add(Dropout(0.3))

  descriptor_model.add(Conv2D(128, 8, padding='valid', use_bias = True, kernel_initializer=init_weights))
  descriptor_model.add(Reshape((128,)))
  return descriptor_model
  
  
def triplet_loss(x):
  output_dim = 128
  a, p, n = x
  _alpha = 1.0 # _alpha is the margin parameter
  positive_distance = K.mean(K.square(a - p), axis=-1)
  negative_distance = K.mean(K.square(a - n), axis=-1)
  return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha), axis = 1)

print('finished')

"""## Denoising patches

We use the DenoiseHPatches class implemented in the read_data.py file, which takes as input the list of sequences to load. It outputs batches where the input data is the noisy image and the label is the clean image, so then we can use a mean absolute error metric (or MSE also works) as loss function. 

Here we take a subset of training and validation sequences by using random.sample (3 sequences for training and 1 for validation data). The purpose of doing so was just to speed-up training for generating faster the output of this notebook. Remove the random.sample function to give the generator all the training data.
"""

denoise_generator = DenoiseHPatches(seqs_train, batch_size=50)
# denoise_generator_val = DenoiseHPatches(random.sample(seqs_test, 5), batch_size=50)
denoise_generator_val = DenoiseHPatches(seqs_test, batch_size=50)

shape = (32, 32, 1) # input image dimension
denoise_model = get_denoise_model(shape)

"""We set number of epochs to 1, tweak it, along with other hyperparameters, to improve the performance of the model."""

printm()

# sgd = keras.optimizers.SGD(lr=0.00001, momentum=0.9, nesterov=True)
adam = keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False)
denoise_model.compile(loss='mean_absolute_error', optimizer=adam, metrics=['mae'])
#denoise_history = denoise_model.fit(x=seqs_train,epochs=1,verbose=2,validation_split=0.25)
denoise_history = denoise_model.fit_generator(generator=denoise_generator, epochs=1, verbose=1, validation_data=denoise_generator_val)
denoise_model.save_weights('denoise_adam_mae_epoch1_lr1e-3_bet0.9_bet0.999_ep1e8.h5')

denoise_model.save_weights('denoise_sgd_mae_epoch1_lr1e-4_momen0.9.h5')





"""You may want to save the weights in your local disk. To do so, use:"""

from google.colab import files
files.download('denoise_sgd_mae_epoch1_lr1e-4_momen0.9.h5')

"""### Visualization of denoising results
To visualize how the denoised patches look, you can run the following function. It returns the noisy patch, the denoised patch in the middle, and the clean patch in the right side.
"""

plot_denoise(denoise_model)

"""## Training a Descriptor Network
Now we train the network that generates the descriptors for the patch. We are going to use the triplet loss, which takes an anchor patch, a negative patch and a positive patch. The idea is to train the network so the descriptors from the anchor and positive patch have a low distance between them, and the negative and anchor patch have a large distance between them. 

In this cell we generate a triplet network, which is a network formed by three copies of the same network. That means that the descriptor model will compute the descriptor for the input `'a'` (anchor), the same descriptor model (with the same weights) will compute the descriptor for the input `'p'` (positive), and again the same model will compute the descriptor for the input `'n'` (negative).
"""

from keras.layers import Lambda
shape = (32, 32, 1) # input dimension
xa = Input(shape=shape, name='a')
xp = Input(shape=shape, name='p')
xn = Input(shape=shape, name='n')
descriptor_model = get_descriptor_model(shape)
ea = descriptor_model(xa)
ep = descriptor_model(xp)
en = descriptor_model(xn)

loss = Lambda(triplet_loss)([ea, ep, en]) # arbitrary loss function as layer (by doing this we define our own loss function)

descriptor_model_trip = Model(inputs = [xa, xp, xn], outputs = loss)
sgd = keras.optimizers.SGD(lr = 0.1)
descriptor_model_trip.compile(loss='mean_absolute_error', optimizer=sgd)

"""Here we use the class HPatches, which loads the corresponding files by using the method read_image_file. It reads the clean patches, which are the ones used for training in this baseline code. The output of read_image_file is a tuple of the form (images, labels), which is passed to the class DataGeneratorDesc. This class is a generator that creates batches of triplets, and each epoch is defined by the number of triplets in the argument `num_triplets`."""

### Descriptor loading and training
# Loading images
hPatches = HPatches(train_fnames = train_fnames, test_fnames = test_fnames)
# Creating training generator
training_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train = 1), num_triplets = 100000)
# Creating validation generator
val_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train = 0), num_triplets = 10000)

"""We plot a random triplet in the form of anchor, positive and negative sample"""

plot_triplet(training_generator)

"""We now train the descriptor model and save the weights afterward."""

history = descriptor_model_trip.fit_generator(generator=training_generator, epochs=10, verbose=1, validation_data=val_generator)
descriptor_model.save_weights('hardnet.h5')

"""## Generating descriptors files for test data 

HPatches benchmark takes as input the descriptors for the test data in a CSV form. This function generates those files by passing it a descriptor model and a denoising model. It performs a first step of denoising the patches, and a second one of computing the descriptor of the denoised patch. If no denoising model is given (variable set to None), the descriptor is computed directly in the noisy patch.
"""

generate_desc_csv(descriptor_model, denoise_model, seqs_test)

"""## Evaluating descriptors in HPatches Benchmark
We first download the official repository for HPatches Benchmark.
"""

!git clone https://github.com/hpatches/hpatches-benchmark

"""Now we will perform the evaluation of three different tasks (Verification, Matching and Evaluation) using the CSV files we generated as input and the `hpatches_eval.py` script. We also print the results using the `hpatches_results.py` script.

### Verification
"""

!python ./hpatches-benchmark/python/hpatches_eval.py --descr-name=custom --descr-dir=/content/keras_triplet_descriptor/out/ --task=verification --delimiter=";"
!python ./hpatches-benchmark/python/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/python/results/ --task=verification

"""### Matching"""

!python ./hpatches-benchmark/python/hpatches_eval.py --descr-name=custom --descr-dir=/content/keras_triplet_descriptor/out/ --task=matching --delimiter=";"
!python ./hpatches-benchmark/python/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/python/results/ --task=matching

"""### Retrieval"""

!python ./hpatches-benchmark/python/hpatches_eval.py --descr-name=custom --descr-dir=/content/keras_triplet_descriptor/out/ --task=retrieval --delimiter=";"
!python ./hpatches-benchmark/python/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/python/results/ --task=retrieval

"""## Compressing and saving the CSV files 

We first compress the directory with all the CSV by using the following command. Remove the `q` option if you want it to output the progress.
"""

# !zip -rq descriptors.zip ./out/custom
!zip -r descriptors.zip ./out/custom

"""The generated .zip is quite large, the method we used for the weights does not work. We have two other methods. First, in the file explorer in the left column we can right-click in the file and then click download. Then, we will see a circle next to the file showing the download progress.

The second way does not require for you to download the files, it save the zip file in your Google Drive account, and you can download it later to your machine if you want. To do so, follow this method (found [here](https://stackoverflow.com/questions/49428332/how-to-download-large-files-like-weights-of-a-model-from-colaboratory)). First run the next cell, and the output will be a link for authentication purposes, and just follow the instructions
"""

from google.colab import auth
from googleapiclient.http import MediaFileUpload
from googleapiclient.discovery import build

auth.authenticate_user()
drive_service = build('drive', 'v3')

def save_file_to_drive(name, path):
  file_metadata = {
    'name': name,
    'mimeType': 'application/octet-stream'
  }

  media = MediaFileUpload(path, 
                          mimetype='application/octet-stream',
                          resumable=True)

  created = drive_service.files().create(body=file_metadata,
                                  media_body=media,
                                  fields='id').execute()

  print('File ID: {}'.format(created.get('id')))

  return created

"""Now we can use the following function to save the file to your drive account. The second argument is the name of the file we want to save, and the first argument the name that will have in your Drive."""

save_file_to_drive('descriptors_save.zip', 'descriptors.zip')